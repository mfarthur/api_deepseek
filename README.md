ðŸ¤– RAG com LLM Local e Base VetorialEste projeto Ã© uma implementaÃ§Ã£o de um sistema de RAG (Retrieval-Augmented Generation) utilizando um modelo de linguagem grande (LLM) rodando localmente com Ollama e uma base de dados vetorial com ChromaDB. A API Ã© construÃ­da com FastAPI.O sistema Ã© capaz de receber perguntas, buscar informaÃ§Ãµes relevantes em uma base de documentos customizada e gerar respostas coesas e contextuais.âœ¨ FuncionalidadesIngestÃ£o de Documentos: Processa arquivos de texto (.txt) para alimentar a base de conhecimento.Limpeza de Texto: Rotinas de prÃ©-processamento para remover ruÃ­dos e formataÃ§Ãµes indesejadas dos textos.Busca Vetorial: Utiliza sentence-transformers e ChromaDB para encontrar os trechos de texto mais relevantes para uma dada pergunta.GeraÃ§Ã£o de Resposta com LLM Local: Integra-se com qualquer modelo suportado pelo Ollama (neste caso, deepseek-v2) para gerar respostas.API Robusta: Uma API feita com FastAPI para servir o modelo e facilitar a integraÃ§Ã£o com frontends.ðŸ› ï¸ Tecnologias UtilizadasBackend: Python 3.8+API: FastAPILLM Local: Ollama (com o modelo deepseek-v2)Base Vetorial: ChromaDBEmbeddings: Sentence TransformersProcessamento de Texto: LangChain (para TextSplitter)ðŸš€ Como Rodar o ProjetoSiga estes passos na ordem correta para configurar e executar a aplicaÃ§Ã£o em seu ambiente local.1. PrÃ©-requisitosAntes de comeÃ§ar, garanta que vocÃª tenha os seguintes softwares instalados:Python 3.8+Ollama: Ã‰ fundamental para rodar o modelo de linguagem localmente. Se ainda nÃ£o o tiver, baixe e instale a partir do site oficial do Ollama.2. ConfiguraÃ§Ã£o InicialEstes comandos preparam o ambiente do projeto. Execute-os na raiz do seu projeto.# 1. Clone este repositÃ³rio (se estiver no GitHub)
git clone https://github.com/seu-usuario/seu-repositorio.git
cd seu-repositorio

# Instale todas as bibliotecas necessÃ¡rias:
pip install -r requirements.txt

# 3. Configure o ambiente
# Crie um arquivo chamado .env na raiz do projeto e cole o seguinte conteÃºdo dentro dele:
# LLM_API_KEY="ollama"
# LLM_API_URL="http://localhost:11434/v1/chat/completions"
# (VocÃª pode criar o arquivo com o comando abaixo no Linux/macOS)
echo 'LLM_API_KEY="ollama"\nLLM_API_URL="http://localhost:11434/v1/chat/completions"' > .env

# 4. Adicione seus documentos
# Coloque todos os seus arquivos de texto (.txt) na pasta /documents.

# 5. Baixe o modelo de linguagem local via Ollama
# Este comando farÃ¡ o download do modelo (pode demorar um pouco) e o deixarÃ¡ pronto para uso.
# Rode este comando uma vez para garantir que o modelo estÃ¡ disponÃ­vel.
ollama pull deepseek-v2
3. Executando a AplicaÃ§Ã£oCom o ambiente configurado, siga esta sequÃªncia para colocar a aplicaÃ§Ã£o no ar.# PASSO 1: IngestÃ£o dos Dados
# Este script lÃª, limpa e vetoriza seus documentos para a base de dados.
# Rode-o uma vez ou sempre que alterar os documentos.
python -m data_processing.ingest

# PASSO 2: Iniciar o Servidor do Modelo (Terminal 1)
# Este comando ativa o LLM. Este terminal precisa ficar aberto.
ollama run deepseek-v2

# PASSO 3: Iniciar a API Backend (Terminal 2)
# Em um NOVO terminal (deixe o terminal do Ollama rodando), inicie o servidor FastAPI.
# Este tambÃ©m precisa ficar aberto.
uvicorn api.main:app --reload
ApÃ³s o Passo 3, sua API estarÃ¡ rodando e acessÃ­vel em http://127.0.0.1:8000.ðŸ”¬ Como TestarExistem duas formas principais de testar o sistema.Teste da AplicaÃ§Ã£o Completa (com LLM)Com os servidores do Ollama e do Uvicorn rodando, abra seu navegador e acesse a documentaÃ§Ã£o interativa da API:http://127.0.0.1:8000/docsExpanda o endpoint POST /api/v1/query, clique em "Try it out" e envie uma pergunta no formato JSON.Teste EspecÃ­fico da Base de Dados (sem LLM)Este teste verifica se a busca vetorial estÃ¡ retornando os chunks de texto corretos para uma pergunta. NÃ£o Ã© necessÃ¡rio ter o Ollama ou o Uvicorn rodando.# Execute o script de teste passando uma pergunta entre aspas
python test_vector_store.py "Qual Ã© a sua pergunta de teste?"
ðŸ“‚ Estrutura do Projeto/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py             # LÃ³gica da API FastAPI e endpoint principal
â”‚   â””â”€â”€ models.py           # Modelos Pydantic para validaÃ§Ã£o de dados
â”œâ”€â”€ core/
â”‚   â””â”€â”€ config.py           # Carregamento e gerenciamento de configuraÃ§Ãµes
â”œâ”€â”€ data_processing/
â”‚   â””â”€â”€ ingest.py           # Script para ingestÃ£o, limpeza e vetorizaÃ§Ã£o de dados
â”œâ”€â”€ documents/
â”‚   â””â”€â”€ (coloque seus .txt aqui)
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ llm_service.py      # ServiÃ§o para se comunicar com o LLM via Ollama
â”‚   â””â”€â”€ vector_store_service.py # ServiÃ§o para interagir com o ChromaDB
â”œâ”€â”€ .env                    # Arquivo de configuraÃ§Ã£o local (NÃƒO ENVIAR PARA O GIT)
â”œâ”€â”€ requirements.txt        # Lista de dependÃªncias Python
â”œâ”€â”€ test_vector_store.py    # Script para testar a busca vetorial
â””â”€â”€ README.md               # Este arquivo
Este README foi gerado em 30 de junho de 2025.
