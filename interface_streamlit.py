# interface_streamlit.py

import streamlit as st
import sys
import os
import asyncio
import traceback  # Para depura√ß√£o de erros

# Garante que o caminho do projeto esteja no sys.path para encontrar os m√≥dulos
sys.path.append(os.path.abspath(os.path.dirname(__file__)))

# Importa os servi√ßos necess√°rios
from services.vector_store_service import vector_store_service
from services.llm_service import llm_service

# --- Configura√ß√£o da P√°gina do Streamlit ---
st.set_page_config(
    page_title="Chat Filos√≥fico",
    page_icon="üèõÔ∏è",
    layout="wide"
)

# --- Barra Lateral (Sidebar) ---
with st.sidebar:
    st.title("Painel de Controlo")
    st.markdown("Ajuste os par√¢metros do seu assistente filos√≥fico.")
    
    st.divider()

    # Slider para controlar o n√∫mero de chunks de contexto
    num_resultados = st.slider(
        "N√∫mero de trechos de contexto (Relev√¢ncia):",
        min_value=1,
        max_value=10,
        value=4,
        help="Selecione quantos trechos relevantes dos textos devem ser usados para formular a resposta."
    )
    
    st.divider()
    
    st.subheader("üèõÔ∏è Sobre a Ferramenta")
    st.info(
        "Este √© um chatbot que utiliza a t√©cnica RAG (Retrieval-Augmented Generation) "
        "para responder a perguntas com base numa vasta cole√ß√£o de textos filos√≥ficos e liter√°rios."
    )

# --- L√≥gica do Chat ---
st.title("üèõÔ∏è Chat Filos√≥fico")
st.markdown("### Converse com os grandes pensadores da hist√≥ria.")

# Inicializa o hist√≥rico do chat na sess√£o do Streamlit
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ol√°! Como posso ajud√°-lo a explorar estes textos filos√≥ficos hoje?"}]

# Exibe as mensagens do hist√≥rico
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Captura a nova pergunta do utilizador
if prompt := st.chat_input("Fa√ßa a sua pergunta..."):
    # Adiciona a mensagem do utilizador ao hist√≥rico e exibe-a
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # --- Inicia o fluxo RAG para gerar a resposta do assistente ---
    with st.chat_message("assistant"):
        with st.spinner("A consultar os textos e a formular uma resposta..."):
            try:
                # 1. BUSCA: Encontra os chunks relevantes
                retrieved_results = vector_store_service.search(prompt, n_results=num_resultados)

                if not retrieved_results:
                    response = "Desculpe, n√£o consegui encontrar informa√ß√µes relevantes sobre este t√≥pico nos documentos dispon√≠veis."
                else:
                    # Extrai os textos dos chunks como contexto
                    context_texts = [doc for doc, meta in retrieved_results]

                    # 2. GERA√á√ÉO: Envia o contexto e a pergunta para o LLM
                    final_answer = asyncio.run(
                        llm_service.get_response(query=prompt, context=context_texts)
                    )

                    # Apenas retorna a resposta (sem exibir fontes)
                    response = final_answer

                # Exibe a resposta final
                st.markdown(response)

                # Adiciona a resposta ao hist√≥rico
                st.session_state.messages.append({"role": "assistant", "content": response})

            except Exception as e:
                print("--- OCORREU UM ERRO ---")
                traceback.print_exc()
                print("------------------------")
                
                error_message = f"Ocorreu um erro inesperado do tipo `{type(e).__name__}`. Por favor, verifique o terminal para mais detalhes."
                st.error(error_message)
                st.session_state.messages.append({"role": "assistant", "content": error_message})
