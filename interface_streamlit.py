# interface_streamlit.py

import streamlit as st
import sys
import os
import re
import asyncio
import traceback # <-- Adicionado para depuraÃ§Ã£o detalhada de erros

# Garante que o caminho do projeto esteja no sys.path para encontrar os mÃ³dulos
# Isto Ã© crucial para que o Streamlit encontre a pasta 'services'
sys.path.append(os.path.abspath(os.path.dirname(__file__)))

# Importa os serviÃ§os necessÃ¡rios
from services.vector_store_service import vector_store_service
from services.llm_service import llm_service

# --- ConfiguraÃ§Ã£o da PÃ¡gina do Streamlit ---
st.set_page_config(
    page_title="Chat FilosÃ³fico",
    page_icon="ðŸ›ï¸",
    layout="wide"
)

# --- Barra Lateral (Sidebar) ---
with st.sidebar:
    st.title("Painel de Controlo")
    st.markdown("Ajuste os parÃ¢metros do seu assistente filosÃ³fico.")
    
    st.divider()

    # Slider para controlar o nÃºmero de chunks de contexto
    num_resultados = st.slider(
        "NÃºmero de trechos de contexto (RelevÃ¢ncia):",
        min_value=1,
        max_value=10,
        value=4,
        help="Selecione quantos trechos relevantes dos textos devem ser usados para formular a resposta."
    )
    
    st.divider()
    
    st.subheader("ðŸ›ï¸ Sobre a Ferramenta")
    st.info(
        "Este Ã© um chatbot que utiliza a tÃ©cnica RAG (Retrieval-Augmented Generation) "
        "para responder a perguntas com base numa vasta coleÃ§Ã£o de textos filosÃ³ficos e literÃ¡rios."
    )

# --- LÃ³gica do Chat ---

st.title("ðŸ›ï¸ Chat FilosÃ³fico")
st.markdown("### Converse com os grandes pensadores da histÃ³ria.")

# Inicializa o histÃ³rico do chat na sessÃ£o do Streamlit
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "OlÃ¡! Como posso ajudÃ¡-lo a explorar estes textos filosÃ³ficos hoje?"}]

# Exibe as mensagens do histÃ³rico
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Captura a nova pergunta do utilizador
if prompt := st.chat_input("FaÃ§a a sua pergunta..."):
    # Adiciona a mensagem do utilizador ao histÃ³rico e exibe-a
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # --- Inicia o fluxo RAG para gerar a resposta do assistente ---
    with st.chat_message("assistant"):
        # Mostra um spinner enquanto a resposta estÃ¡ a ser gerada
        with st.spinner("A consultar os textos e a formular uma resposta..."):
            try:
                # 1. BUSCA (Retrieval): Encontra os chunks relevantes na base de dados
                retrieved_results = vector_store_service.search(prompt, n_results=num_resultados)
                
                if not retrieved_results:
                    response = "Desculpe, nÃ£o consegui encontrar informaÃ§Ãµes relevantes sobre este tÃ³pico nos documentos disponÃ­veis."
                else:
                    # Extrai apenas o texto dos chunks para usar como contexto
                    context_texts = [doc for doc, meta in retrieved_results]
                    
                    # 2. GERAÃ‡ÃƒO (Generation): Envia o contexto e a pergunta para o LLM
                    # Como o nosso llm_service Ã© assÃ­ncrono, usamos asyncio.run()
                    final_answer = asyncio.run(
                        llm_service.get_response(query=prompt, context=context_texts)
                    )
                    
                    # ConstrÃ³i a resposta final com as fontes
                    fontes_texto = "\n\n---\n**Fontes Consultadas:**\n"
                    fontes_unicas = set()
                    for doc, meta in retrieved_results:
                        fonte = meta.get('fonte', 'Desconhecida')
                        if fonte not in fontes_unicas:
                            fontes_unicas.add(fonte)
                            fontes_texto += f"- *{fonte}*\n"
                    
                    response = final_answer + fontes_texto

                # Exibe a resposta final
                st.markdown(response)
                
                # Adiciona a resposta do assistente ao histÃ³rico
                st.session_state.messages.append({"role": "assistant", "content": response})

            except Exception as e:
                # --- MELHORIA: DepuraÃ§Ã£o de Erros ---
                # Imprime o erro completo no terminal para diagnÃ³stico
                print("--- OCORREU UM ERRO ---")
                traceback.print_exc()
                print("-------------------------")
                
                # Exibe uma mensagem de erro mais informativa na interface
                error_message = f"Ocorreu um erro inesperado do tipo `{type(e).__name__}`. Por favor, verifique o terminal para mais detalhes."
                st.error(error_message)
                st.session_state.messages.append({"role": "assistant", "content": error_message})
